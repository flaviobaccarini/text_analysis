from binary_classifier.vectorize_data import vectorize_X_data_lr, vectorize_X_data_tf
from binary_classifier.vectorize_data import calculate_max_len, get_vocabulary
from binary_classifier.vectorize_data import MeanEmbeddingVectorizer
from binary_classifier.vectorize_data import init_vector_layer, tocat_encode_labels
from binary_classifier.vectorize_data import flatten_unique_voc
import numpy as np
import nltk
from hypothesis import strategies as st
from hypothesis import given
import string
import random
from gensim.models import Word2Vec
import tensorflow as tf

def test_vocabulary():
    '''
    Test function to test the working of the get_vocabulary function.
    The input to get_vocabulary should be a list of sentences containing all 
    the sentences to extrapolate the vocabulary.
    The output is a list of lists containg all the single words.
    '''
    all_words = ['hello world', 'what a beautiful day',
                'random string', 'another random text', 'a little bit of fantasy',
                'try with this','1 + 1 is equal to 2', '1 + 1 = 2']

    sentences = get_vocabulary(all_words)

    assert(len(sentences) == 8) # total nr. of sentences == 8
    nr_of_words = 0
    for sentence in sentences:
        nr_of_words += len(sentence)
    assert(nr_of_words == 31) # total nr of words == 31


@given(list_words = st.lists(st.text(max_size = 10)))
def test_voc_rand_text(list_words):
    '''
    Test function to test the working of the get_vocabulary function
    with some random text data generated by the strategies.
    The input to get_vocabulary should be a list of sentences containing all 
    the vocabulary.
    The output is a list containg all the single words.
    The output could also be a unique vocabulary, so words that
    are not repated.
    '''
    
    sentences = get_vocabulary(list_words)

    assert(len(sentences) == len(list_words))

    nr_of_words = 0
    for sentence in sentences:
        nr_of_words += (len(sentence))

def test_flatten_vocabulary():
    '''
    Test function to test the working of the flatten_unique_voc function.
    The input to flatten_unique_voc should be a list of lists
    of strings containing all the sentences tokenized.
    The output is a single list of strings containing 
    all the unique vocabulary words tokenized.
    '''
    all_words = ['hello world', 'what a beautiful day',
                'random string', 'another random text', 'a little bit of fantasy',
                'try with this','1 + 1 is equal to 2', '1 + 1 = 2']

    sentences = get_vocabulary(all_words)

    voc_unique = flatten_unique_voc(sentences)
    assert(len(voc_unique) == 24) # only 24 unique words in the sentences

@given(list_words = st.lists(st.text(max_size = 10)))
def test_flatten_rand_text(list_words):
    '''
    Test function to test the working of the get_vocabulary function
    with some random text data generated by the strategies.
    The input to get_vocabulary should be a list of sentences containing all 
    the vocabulary.
    The output is a list containg all the single words.
    The output could also be a unique vocabulary, so words that
    are not repated.
    '''
    
    sentences = get_vocabulary(list_words)

    voc_unique = flatten_unique_voc(sentences)

    word_tokenize_uniq = [nltk.word_tokenize(word) for word in list_words]
    flat_list = [item for sublist in word_tokenize_uniq for item in sublist]
    uniq_words = " ".join(set(flat_list))
    assert(len(uniq_words.split()) ==  len(voc_unique))

def test_max_len():
    '''
    Test function to test the working of the calculate_max_len function.
    The input to calculate_max_len should be a list of sentences that the 
    user wants to count the length.
    The output is the max length to pass for the creation of the model.
    The max length is equal to the mean of the word counts + 
    2*standard deviation of the word counts.
    '''
    random_text = ['another one bites the dust', 'another string', 'word',
                    'four characters for me', 'awful string']

    maxlen = calculate_max_len(random_text)
    # the mean of the word counts in random_text
    # is 2.8 and the standard deviation is 1.47
    # so we expect that maxlen is equal to 6
    assert(maxlen == 6)

# BUG: QUESTA FUNZIONE QUI SOTTO POTREBBE DARE QUALCHE PROBLEMA 
# IN QUANTO LE DUE QUANTITÀ CALCOLATE POTREBBERO NON ESSERE COSÌ TANTO VICINE
# PER UN MERO FATTO DI STATISTICA. QUINDI DECIDERE COSA FARE, SE TENERE E MAGARI PROVARE
# A MODIFICARLA OPPURE TOGLIERLA
@given(mean = st.floats(min_value = 0., max_value = 20), std = st.floats(min_value=0., max_value=5.))
def test_max_len_randnumb(mean, std):
    '''
    Test function to test the working of the calculate_max_len function
    with some random mean and standard deviation.
    The input to calculate_max_len should be a list of sentences that the 
    user wants to count the length.
    The output is the max length to pass for the creation of the model.
    The max length is equal to the mean of the word counts + 
    2*standard deviation of the word counts.

    In the assertion it's used the function np.isclose with a tolerance of 2,
    because of a possible statistic problem, when generating the random length
    with the Gaussian distribution (used only 100 samples).
    '''
    samples = 100
    random_length = np.random.normal(loc = mean, scale = std, size = (samples,))
    sentences = [' '.join(random.choices(string.ascii_letters + 
                             string.digits, k= int(np.round(size)))) for size in random_length]
    maxlen = calculate_max_len(sentences)

    assert(np.isclose(maxlen, np.round(mean + 2*std), atol=2.0))


def test_encode_labels():
    '''
    Test function to test the working of tocat_encode_labels function.
    The function takes as input the labels, that the user wants to be 
    encoded in numbers.
    The output is an numpy array with the same size of the initial list
    composed by numbers from 0 to n-1 where n is the number of the classes.
    The function returns also the name classes associated to the numbers.
    '''
    unique_labels = ['real', 'fake']
    labels = [unique_labels[random.randrange(len(unique_labels))] for _ in range(100)]
    indices = {'real': [index for (index, label) in enumerate(labels) if label == 'real'],
               'fake': [index for (index, label) in enumerate(labels) if label == 'fake']}
    
    y_categorical, classes = tocat_encode_labels(labels, classes = True)

    assert(len(y_categorical) == 100)
    assert(set(classes) == {'real', 'fake'})
    assert(type(y_categorical) == np.ndarray)
    assert(set(y_categorical) == {0, 1})

    y_categorical_0index = np.where(y_categorical == 0)
    y_categorical_1index = np.where(y_categorical == 1)

    assert((np.array(indices[classes[0]]) == y_categorical_0index).all())
    assert((np.array(indices[classes[1]]) == y_categorical_1index).all())

@given(unique_labels = st.lists(st.sampled_from(string.ascii_letters),
                                 min_size = 1, max_size=10, unique=True))
def test_encode_random_labels(unique_labels):
    '''
    Test function to test the working of tocat_encode_labels function
    for multiple labels (not just two).
    The function takes as input the labels, that the user wants to be 
    encoded in numbers.
    The output is an numpy array with the same size of the initial list
    composed by numbers from 0 to n-1 where n is the number of the classes.
    The function returns also the name classes associated to the numbers.
    '''
    labels_list = [unique_labels[random.randrange(len(unique_labels))] for _ in range(500)]
    indices = {}
    for label in unique_labels:
        indices[label] = [index for (index, lab) in enumerate(labels_list) if label == lab]
    y_categorical, classes = tocat_encode_labels(labels_list, classes=True)

    assert(len(y_categorical) == 500)
    assert(type(y_categorical) == np.ndarray)
    if not set(classes).issubset(set(unique_labels)):
        print(classes, unique_labels)
    for y_cat in y_categorical:
        assert(type(y_cat.item()) == int)

    y_indices = {}
    for y_cat in np.unique(y_categorical):
        y_indices[y_cat] = np.where(y_categorical == y_cat) 

    for class_, y_cat in zip(classes, np.unique(y_categorical)):
        assert((np.array(indices[class_]) == y_indices[y_cat]).all())


def test_shape_mean_emb_vect():
    '''
    Test function to test the working of the MeanEmbeddingVectorizer class.
    The MeanEmbeddingVectorizer class makes an average of the vector words
    in the sentence given to the transformer. The vector words are passed 
    to the class by a dictionary. The dictionary is the result
    of a Word2Vec model.

    In this test function the shape is tested: the output shape should be
    (number of sentences, embedding vector size)
    '''
    random_text_voc = ['hello world', 'hello my name is Tom',
                    'Tom loves catching fishes', 'random word in a random string',
                    'hello what a beautiful day',
                    'the sky is blue']
    vocabulary = get_vocabulary(random_text_voc)
    embedding_vector_size = 10
    min_count_words = 2
    random_state = 42

    modelw2v = Word2Vec(vocabulary, vector_size=embedding_vector_size, window=5,
                         min_count=min_count_words, workers=1, seed = random_state)   
    w2v = dict(zip(modelw2v.wv.index_to_key, modelw2v.wv.vectors)) 
    
    text_to_transform = ['hello world, my name is Flavio',
                         'in my free time i love catching fishes']

    X_tok = [nltk.word_tokenize(words) for words in text_to_transform]  

    mean_emb_vect = MeanEmbeddingVectorizer(w2v)
    X_vectors_w2v = mean_emb_vect.transform(X_tok)
    assert(np.shape(X_vectors_w2v) == (len(text_to_transform), embedding_vector_size)) # two sentences 


def test_value_mean_emb_vect():
    '''
    Test function to test the working of the MeanEmbeddingVectorizer class.
    The MeanEmbeddingVectorizer class makes an average of the vector words
    in the sentence given to the transformer. The vector words are passed 
    to the class by a dictionary. The dictionary is the result
    of a Word2Vec model.
    
    In this test function the correct values of different 
    sentences are tested: the output should be the average of the words 
    contained in the Word2Vec dictionary, or a zeros array if there is no match
    between words from sentence and words from dictionary.
    '''

    random_text_voc = ['hello world', 'hello my name is Tom',
                    'Tom loves catching fishes', 'random word in a random string',
                    'hello what a beautiful day',
                    'the sky is blue']
    vocabulary = get_vocabulary(random_text_voc)
    embedding_vector_size = 10
    min_count_words = 1
    random_state = 42

    modelw2v = Word2Vec(vocabulary, vector_size=embedding_vector_size, window=5,
                         min_count=min_count_words, workers=1, seed = random_state)   
    w2v = dict(zip(modelw2v.wv.index_to_key, modelw2v.wv.vectors)) 
    
    text_to_transform = ['hello',
                         'Flavio',
                         'hello world my name is Tom']

    X_tok = [nltk.word_tokenize(words) for words in text_to_transform]  

    mean_emb_vect = MeanEmbeddingVectorizer(w2v)
    X_vectors_w2v = mean_emb_vect.transform(X_tok)

    assert( np.isclose(X_vectors_w2v[0], w2v['hello']).all() )
    assert( (X_vectors_w2v[1] == np.zeros(shape=(1, embedding_vector_size))).all() )
    assert( np.isclose(X_vectors_w2v[2], np.mean([w2v['hello'], w2v['world'], w2v['my'],
                                        w2v['name'], w2v['is'], w2v['Tom']], axis = 0)).all() )


def test_value_meanembvec_count_words():
    '''
    Test function to test the working of the MeanEmbeddingVectorizer class.
    The MeanEmbeddingVectorizer class makes an average of the vector words
    in the sentence given to the transformer. The vector words are passed 
    to the class by a dictionary. The dictionary is the result
    of a Word2Vec model.
    
    In this test function the correct values of different 
    sentences are tested: the output should be the average of the words 
    contained in the Word2Vec dictionary, or a zeros array if there is no match
    between words from sentence and words from dictionary.

    In this particular test function the minimun count words is set to 2: it means
    that only words with two or more occurencies will make the Word2Key dictionary.
    '''

    random_text_voc = ['hello world', 'hello my name is Tom',
                    'Tom loves catching fishes', 'random word in a random string',
                    'hello what a beautiful day',
                    'the sky is blue']
    vocabulary = get_vocabulary(random_text_voc)
    embedding_vector_size = 10
    min_count_words = 2 # the word occurencies must be at least 2 -> if not: no in the vocabulary
    random_state = 42

    modelw2v = Word2Vec(vocabulary, vector_size=embedding_vector_size, window=5,
                         min_count=min_count_words, workers=1, seed = random_state)   
    w2v = dict(zip(modelw2v.wv.index_to_key, modelw2v.wv.vectors)) 
    
    text_to_transform = ['hello',
                         'Flavio',
                         'hello world my name is Tom',
                         "I'm Flavio, I catch fishes"]

    X_tok = [nltk.word_tokenize(words) for words in text_to_transform]  

    mean_emb_vect = MeanEmbeddingVectorizer(w2v)
    X_vectors_w2v = mean_emb_vect.transform(X_tok)

    zero_array = np.zeros(shape=(embedding_vector_size))

    assert( np.isclose(X_vectors_w2v[0], w2v['hello']).all() )
    assert( (X_vectors_w2v[1] == zero_array).all() )
    assert( np.isclose(X_vectors_w2v[2], np.mean([w2v['hello'], w2v['is'],
                                         w2v['Tom']], axis = 0)).all() )
    assert( (X_vectors_w2v[3] == zero_array).all() )


def test_vect_X_lr():
    '''
    Test function to test the correct values for the vectorize_X_data_lr function.

    vectorize_X_data_lr takes as input the text the user wants to vectorize, which 
    is a list of strings and a model of Word2Vec already trained.

    The output is the vectorized text.
    '''
    random_text_voc = ['hello world', 'hello my name is Tom',
                    'Tom loves catching fishes', 'random word in a random string',
                    'hello what a beautiful day',
                    'the sky is blue']
    vocabulary = get_vocabulary(random_text_voc)
    embedding_vector_size = 10
    min_count_words = 1
    random_state = 42

    modelw2v = Word2Vec(vocabulary, vector_size=embedding_vector_size, window=5,
                         min_count=min_count_words, workers=1, seed = random_state)   
    text_to_vectorize= ['hello',
                         'Flavio',
                         'hello world my name is Tom',
                         "I'm Flavio, I catch fishes"]
    
    
    X_vector = vectorize_X_data_lr(text_to_vectorize, modelw2v)
    w2v = dict(zip(modelw2v.wv.index_to_key, modelw2v.wv.vectors)) 
    zero_array = np.zeros(shape=(embedding_vector_size))


    mean_third_phrase = np.mean([w2v['hello'],  w2v['my'],
                                w2v['world'], w2v['name'],    
                                w2v['is'],w2v['Tom']], axis = 0)

    assert( np.isclose(X_vector[0], w2v['hello']).all() )
    assert( (X_vector[1] ==  zero_array).all() )
    assert( np.isclose(X_vector[2], mean_third_phrase).all() )
    assert( np.isclose(X_vector[3], w2v['fishes']).all() )


@given(embedding_vector_size = st.integers(min_value = 1, max_value = 40),
       text_to_vectorize = st.lists(st.sampled_from(''.join(string.ascii_letters)), min_size = 1, max_size = 40))
def test_shape_vect_X_lr(embedding_vector_size, text_to_vectorize):
    '''
    Test function to test the correct shape of the outout
    for the vectorize_X_data_lr function.

    vectorize_X_data_lr takes as input the text the user wants to vectorize, which 
    is a list of strings and a model of Word2Vec already trained.

    The output is the vectorized text with a precise shape, that
    is given by (number of strings in the inital list, embedding vector size).
    '''

    random_text_voc = ['hello world', 'hello my name is Tom',
                    'Tom loves catching fishes', 'random word in a random string',
                    'hello what a beautiful day',
                    'the sky is blue']
    vocabulary = get_vocabulary(random_text_voc)
    min_count_words = 1
    random_state = 42

    modelw2v = Word2Vec(vocabulary, vector_size=embedding_vector_size, window=5,
                         min_count=min_count_words, workers=1, seed = random_state)   
    
    X_vector = vectorize_X_data_lr(text_to_vectorize, modelw2v)
    
    assert( X_vector.shape == (len(text_to_vectorize), embedding_vector_size))


def test_vector_layer():
    '''
    Test function to test the working of the init_vector_layer function.
    In this test function the layer vocabulary is the one
    given by the user is tested.
    Then the vector layer has the correct type (TextVectoriation layer). 
    '''
    random_text_voc = ['hello world', 'hello my name is Tom',
                    'Tom loves catching fishes', 'random word in a random string',
                    'hello what a beautiful day',
                    'the sky is blue']
    vocabulary = get_vocabulary(random_text_voc)
    uniq_voc = flatten_unique_voc(vocabulary)
    maxlen = calculate_max_len(random_text_voc)

    vector_layer = init_vector_layer(maxlen, uniq_voc)
    
    voc_from_layer = vector_layer.get_vocabulary(include_special_tokens = False)
    voc_size = vector_layer.vocabulary_size()

    
    assert((uniq_voc == voc_from_layer).all())
    assert(len(uniq_voc) + 2 == voc_size) # +2 because of special tokes
    assert(type(vector_layer) == tf.keras.layers.TextVectorization)


@given(maxlen = st.integers(min_value = 1, max_value = 40),
       text_to_vectorize = st.lists(st.sampled_from(''.join(string.ascii_letters)), min_size = 1, max_size = 40))
def test_shape_vectorize_X_tf(maxlen, text_to_vectorize):
    '''
    Test function to test the correct shape of the outout
    for the vectorize_X_data_tf function.

    vectorize_X_data_tf takes as input the text the user wants to vectorize, which 
    is a string and the tensorflow vectorization layer.

    The output is the vectorized text with a precise shape, that
    is given by (maxlen,).
    Eventually some padding/truncation operations are
    performed by the layer to have arrays with the same shape.
    '''
    random_text_voc = ['hello world', 'hello my name is Tom',
                    'Tom loves catching fishes', 'random word in a random string',
                    'hello what a beautiful day',
                    'the sky is blue']
    vocabulary = get_vocabulary(random_text_voc)
    uniq_voc = flatten_unique_voc(vocabulary)

    vector_layer = init_vector_layer(maxlen, uniq_voc)

    vector = tf.stack([vectorize_X_data_tf(text, vector_layer) for text in text_to_vectorize])

    assert(vector.shape == (len(text_to_vectorize), maxlen))


def test_value_vectorize_X_tf():
    '''
    Test function to test the correct values of the outout
    for the vectorize_X_data_tf function.

    vectorize_X_data_tf takes as input the text the user wants to vectorize, which 
    is a string and the tensorflow vectorization layer.

    The output is the vectorized text. 

    If there is a match between the words to vectorize and the words
    stored in the vocabulary, the word is represented by the index number
    of the same word in the vocabulary.

    If the word in the text to vectorize is not present in the vocabulary,
    the vectorization layer represents this word with "1".

    At the end of the sentences there could be some zeros to pad the sequences.
    '''
    random_text_voc = ['hello world', 'hello my name is Tom',
                    'Tom loves catching fishes', 'random word in a random string',
                    'hello what a beautiful day',
                    'the sky is blue']
    vocabulary = get_vocabulary(random_text_voc)
    uniq_voc = flatten_unique_voc(vocabulary)
    maxlen = calculate_max_len(random_text_voc)

    vector_layer = init_vector_layer(maxlen, uniq_voc)

    text_to_vectorize = ['hello',
                        'my name is Flavio',
                        '33 trentini']

    vector = tf.stack([vectorize_X_data_tf(text, vector_layer) for text in text_to_vectorize])

    assert(vector.dtype == tf.int32) # the type is int 32 bit

    # now check the value assigned to each word
    complete_voc = vector_layer.get_vocabulary()

    # we expect the number of 'hello' in vector (vector[0][0])
    # is equal to the index of the 'hello' word in the vocabulary from the layer 
    assert(vector[0][0] == complete_voc.index('hello'))
    # we expect that vector[0] is equal to = index('hello'),0,0,0,0,0,0
    # this is the padding operation
    hello_in_indices = np.array(complete_voc.index('hello'))
    hello_in_indices = hello_in_indices.reshape((1,))
    zero_array = np.zeros(dtype = int, shape = (maxlen - hello_in_indices.size,))
    hello_in_indices = np.concatenate((hello_in_indices, zero_array), axis =0)
    assert((np.array(vector[0]) == hello_in_indices).all())

    indices_vector_two = np.array([complete_voc.index('my'),
               complete_voc.index('name'),
               complete_voc.index('is'),
               1]) # the one is because the 'Flavio' word (which is not inside the vocabulary)
    zero_array = np.zeros(dtype = int, shape = (maxlen - indices_vector_two.size))
    indices_vector_two = np.concatenate((indices_vector_two, zero_array), axis = 0)

    assert((np.array(vector[1]) == indices_vector_two).all())

    # '33' 'trentini' are not words present in the vocabulary -> 1 to represent them
    indices_vector_three = np.array([1, 1]) 
    zero_array = np.zeros(dtype = int, shape = (maxlen - indices_vector_three.size,))
    indices_vector_three = np.concatenate((indices_vector_three, zero_array), axis = 0)

    assert((np.array(vector[2]) == indices_vector_three).all())
