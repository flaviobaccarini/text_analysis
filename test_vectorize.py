from binary_classifier.vectorize_data import vectorize_X_data_lr, vectorize_X_data_tf
from binary_classifier.vectorize_data import calculate_max_len, get_vocabulary
from binary_classifier.vectorize_data import MeanEmbeddingVectorizer
from binary_classifier.vectorize_data import init_vector_layer, tocat_encode_labels
import numpy as np
import nltk

from hypothesis import strategies as st
from hypothesis import given
import string
import random

def test_vocabulary():
    '''
    Test function to test the working of the get_vocabulary function.
    The input to get_vocabulary should be a list of sentences containing all 
    the vocabulary.
    The output is a list containg all the single words.
    The output could also be a unique vocabulary, so words that
    are not repated.
    '''
    all_words = ['hello world', 'what a beautiful day',
                'random string', 'another random text', 'a little bit of fantasy',
                'try with this','1 + 1 is equal to 2', '1 + 1 = 2']

    voc = get_vocabulary(all_words)
    assert(len(voc) == 31) # total nr. of words == 31
    assert(len(np.unique(voc)) == 24) # only 24 unique words

    voc_unique = get_vocabulary(all_words, unique = True)
    assert(len(voc_unique) == 24)

@given(list_words = st.lists(st.text(max_size = 10)))
def test_voc_rand_text(list_words):
    '''
    Test function to test the working of the get_vocabulary function
    with some random text data generated by the strategies.
    The input to get_vocabulary should be a list of sentences containing all 
    the vocabulary.
    The output is a list containg all the single words.
    The output could also be a unique vocabulary, so words that
    are not repated.
    '''
    
    voc = get_vocabulary(list_words)
    nr_of_words = 0
    for item in list_words:
        nr_of_words += (len(nltk.word_tokenize(item)))
  
    assert(len(voc) == nr_of_words)

    voc_unique = get_vocabulary(list_words, unique = True)

    word_tokenize_uniq = [nltk.word_tokenize(word) for word in list_words]
    flat_list = [item for sublist in word_tokenize_uniq for item in sublist]
    uniq_words = " ".join(set(flat_list))
    assert(len(uniq_words.split()) ==  len(voc_unique))


def test_max_len():
    '''
    Test function to test the working of the calculate_max_len function.
    The input to calculate_max_len should be a list of sentences that the 
    user wants to count the length.
    The output is the max length to pass for the creation of the model.
    The max length is equal to the mean of the word counts + 
    2*standard deviation of the word counts.
    '''
    random_text = ['another one bites the dust', 'another string', 'word',
                    'four characters for me', 'awful string']

    maxlen = calculate_max_len(random_text)
    # the mean of the word counts in random_text
    # is 2.8 and the standard deviation is 1.47
    # so we expect that maxlen is equal to 6
    assert(maxlen == 6)

# BUG: QUESTA FUNZIONE QUI SOTTO POTREBBE DARE QUALCHE PROBLEMA 
# IN QUANTO LE DUE QUANTITÀ CALCOLATE POTREBBERO NON ESSERE COSÌ TANTO VICINE
# PER UN MERO FATTO DI STATISTICA. QUINDI DECIDERE COSA FARE, SE TENERE E MAGARI PROVARE
# A MODIFICARLA OPPURE TOGLIERLA
@given(mean = st.floats(min_value = 0., max_value = 20), std = st.floats(min_value=0., max_value=5.))
def test_max_len_randnumb(mean, std):
    '''
    Test function to test the working of the calculate_max_len function
    with some random mean and standard deviation.
    The input to calculate_max_len should be a list of sentences that the 
    user wants to count the length.
    The output is the max length to pass for the creation of the model.
    The max length is equal to the mean of the word counts + 
    2*standard deviation of the word counts.

    In the assertion it's used the function np.isclose with a tolerance of 2,
    because of a possible statistic problem, when generating the random length
    with the Gaussian distribution (used only 100 samples).
    '''
    samples = 100
    random_length = np.random.normal(loc = mean, scale = std, size = (samples,))
    sentences = [' '.join(random.choices(string.ascii_letters + 
                             string.digits, k= int(np.round(size)))) for size in random_length]
    maxlen = calculate_max_len(sentences)

    assert(np.isclose(maxlen, np.round(mean + 2*std), atol=2.0))


def test_encode_labels():
    '''
    Test function to test the working of tocat_encode_labels function.
    The function takes as input the labels, that the user wants to be 
    encoded in numbers.
    The output is an numpy array with the same size of the initial list
    composed by numbers from 0 to n-1 where n is the number of the classes.
    The function returns also the name classes associated to the numbers.
    '''
    unique_labels = ['real', 'fake']
    labels = [unique_labels[random.randrange(len(unique_labels))] for _ in range(100)]
    indices = {'real': [index for (index, label) in enumerate(labels) if label == 'real'],
               'fake': [index for (index, label) in enumerate(labels) if label == 'fake']}
    
    y_categorical, classes = tocat_encode_labels(labels)

    assert(len(y_categorical) == 100)
    assert(set(classes) == {'real', 'fake'})
    assert(type(y_categorical) == np.ndarray)
    assert(set(y_categorical) == {0, 1})

    y_categorical_0index = np.where(y_categorical == 0)
    y_categorical_1index = np.where(y_categorical == 1)

    assert((np.array(indices[classes[0]]) == y_categorical_0index).all())
    assert((np.array(indices[classes[1]]) == y_categorical_1index).all())

@given(unique_labels = st.lists(st.sampled_from(('A', 'B', 'C', 'D', 'E')),
                                 min_size = 1, max_size=10, unique=True))
def test_encode_random_labels(unique_labels):
    '''
    Test function to test the working of tocat_encode_labels function
    for multiple labels (not just two).
    The function takes as input the labels, that the user wants to be 
    encoded in numbers.
    The output is an numpy array with the same size of the initial list
    composed by numbers from 0 to n-1 where n is the number of the classes.
    The function returns also the name classes associated to the numbers.
    '''
    labels_list = [unique_labels[random.randrange(len(unique_labels))] for _ in range(500)]
    indices = {}
    for label in unique_labels:
        indices[label] = [index for (index, lab) in enumerate(labels_list) if label == lab]
    y_categorical, classes = tocat_encode_labels(labels_list)

    assert(len(y_categorical) == 500)
    assert(type(y_categorical) == np.ndarray)
    if not set(classes).issubset(set(unique_labels)):
        print(classes, unique_labels)
    for y_cat in y_categorical:
        assert(type(y_cat.item()) == int)

    y_indices = {}
    for y_cat in np.unique(y_categorical):
        y_indices[y_cat] = np.where(y_categorical == y_cat) 

    for class_, y_cat in zip(classes, np.unique(y_categorical)):
        assert((np.array(indices[class_]) == y_indices[y_cat]).all())
